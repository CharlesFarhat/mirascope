class Call(Protocol):
{% for provider in providers %}
    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[False] = False,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: None = None,
        output_parser: None = None,
        json_mode: bool = False,
        client: {{provider.same_sync_and_async_base_client}} | None = None,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> LLMFunctionDecorator[
        {{provider.base_dynamic_config}},
        {{provider.async_base_dynamic_config}},
        CallResponse[{{provider.response}}, {{provider.base_tool}}],
        CallResponse[{{provider.response}}, {{provider.base_tool}}],
    ]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[False] = False,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: None = None,
        output_parser: None = None,
        json_mode: bool = False,
        client: {{provider.async_base_client}} = ...,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> AsyncLLMFunctionDecorator[
        {{provider.async_base_dynamic_config}}, CallResponse[{{provider.response}}, {{provider.base_tool}}]
    ]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[False] = False,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: None = None,
        output_parser: None = None,
        json_mode: bool = False,
        client: {{provider.sync_base_client}} = ...,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> SyncLLMFunctionDecorator[
        {{provider.base_dynamic_config}},
        CallResponse[{{provider.response}}, {{provider.base_tool}}],
    ]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[False] = False,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: None = None,
        output_parser: Callable[
            [
                CallResponse[{{provider.response}}, {{provider.base_tool}}],
            ],
            _ParsedOutputT,
        ],
        json_mode: bool = False,
        client: {{provider.same_sync_and_async_base_client}} | None = None,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> LLMFunctionDecorator[
        {{provider.base_dynamic_config}}, {{provider.async_base_dynamic_config}}, _ParsedOutputT, _ParsedOutputT
    ]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[False] = False,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: None = None,
        output_parser: Callable[
            [
                CallResponse[{{provider.response}}, {{provider.base_tool}}],
            ],
            _ParsedOutputT,
        ],
        json_mode: bool = False,
        client: {{provider.async_base_client}} = ...,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> AsyncLLMFunctionDecorator[{{provider.async_base_dynamic_config}}, _ParsedOutputT]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[False] = False,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: None = None,
        output_parser: Callable[
            [
                CallResponse[{{provider.response}}, {{provider.base_tool}}],
            ],
            _ParsedOutputT,
        ],
        json_mode: bool = False,
        client: {{provider.sync_base_client}} = ...,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> SyncLLMFunctionDecorator[{{provider.base_dynamic_config}}, _ParsedOutputT]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[False] = False,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: None = None,
        output_parser: Callable[[CallResponseChunk[{{provider.response_chunk}}]], _ParsedOutputT],
        json_mode: bool = False,
        client: {{provider.same_sync_and_async_base_client}}
        | {{provider.sync_base_client}}
        | {{provider.async_base_client}}
        | None = None,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> NoReturn: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[True] | StreamConfig = True,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: None = None,
        output_parser: None = None,
        json_mode: bool = False,
        client: {{provider.same_sync_and_async_base_client}} | None = None,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> LLMFunctionDecorator[
        {{provider.base_dynamic_config}}, {{provider.async_base_dynamic_config}}, {{provider.base_stream}}, {{provider.base_stream}}
    ]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[True] | StreamConfig = True,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: None = None,
        output_parser: None = None,
        json_mode: bool = False,
        client: {{provider.async_base_client}} = ...,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> AsyncLLMFunctionDecorator[{{provider.async_base_dynamic_config}}, {{provider.base_stream}}]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[True] | StreamConfig = True,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: None = None,
        output_parser: None = None,
        json_mode: bool = False,
        client: {{provider.sync_base_client}} = ...,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> SyncLLMFunctionDecorator[{{provider.base_dynamic_config}}, {{provider.base_stream}}]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[True] | StreamConfig = True,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: None = None,
        output_parser: Callable[[CallResponseChunk[{{provider.response_chunk}}]], _ParsedOutputT],
        json_mode: bool = False,
        client: {{provider.same_sync_and_async_base_client}}
        | {{provider.sync_base_client}}
        | {{provider.async_base_client}}
        | None = None,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> NoReturn: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[True] | StreamConfig = True,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: None = None,
        output_parser: Callable[
            [
                CallResponse[{{provider.response}}, {{provider.base_tool}}],
            ],
            _ParsedOutputT,
        ],
        json_mode: bool = False,
        client: {{provider.same_sync_and_async_base_client}}
        | {{provider.sync_base_client}}
        | {{provider.async_base_client}}
        | None = None,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> NoReturn: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[False] = False,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: type[_ResponseModelT],
        output_parser: None = None,
        json_mode: bool = False,
        client: {{provider.same_sync_and_async_base_client}} | None = None,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> LLMFunctionDecorator[
        {{provider.base_dynamic_config}}, {{provider.async_base_dynamic_config}}, _ResponseModelT, _ResponseModelT
    ]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[False] = False,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: type[_ResponseModelT],
        output_parser: None = None,
        json_mode: bool = False,
        client: {{provider.async_base_client}} = ...,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> AsyncLLMFunctionDecorator[{{provider.async_base_dynamic_config}}, _ResponseModelT]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[False] = False,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: type[_ResponseModelT],
        output_parser: None = None,
        json_mode: bool = False,
        client: {{provider.sync_base_client}} = ...,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> SyncLLMFunctionDecorator[{{provider.base_dynamic_config}}, _ResponseModelT]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[False] = False,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: type[_ResponseModelT],
        output_parser: Callable[[_ResponseModelT], _ParsedOutputT],
        json_mode: bool = False,
        client: {{provider.same_sync_and_async_base_client}} | None = None,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> LLMFunctionDecorator[
        {{provider.base_dynamic_config}}, {{provider.async_base_dynamic_config}}, _ParsedOutputT, _ParsedOutputT
    ]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[False] = False,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: type[_ResponseModelT],
        output_parser: Callable[[_ResponseModelT], _ParsedOutputT],
        json_mode: bool = False,
        client: {{provider.async_base_client}} = ...,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> AsyncLLMFunctionDecorator[{{provider.async_base_dynamic_config}}, _ParsedOutputT]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[False] = False,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: type[_ResponseModelT],
        output_parser: Callable[[_ResponseModelT], _ParsedOutputT],
        json_mode: bool = False,
        client: {{provider.sync_base_client}} = ...,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> SyncLLMFunctionDecorator[{{provider.base_dynamic_config}}, _ParsedOutputT]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[True] | StreamConfig,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: type[_ResponseModelT],
        output_parser: None = None,
        json_mode: bool = False,
        client: {{provider.same_sync_and_async_base_client}} | None = None,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> LLMFunctionDecorator[
        {{provider.base_dynamic_config}},
        {{provider.async_base_dynamic_config}},
        Iterable[_ResponseModelT],
        AsyncIterable[_ResponseModelT],
    ]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[True] | StreamConfig,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: type[_ResponseModelT],
        output_parser: None = None,
        json_mode: bool = False,
        client: {{provider.async_base_client}} = ...,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> AsyncLLMFunctionDecorator[
        {{provider.async_base_dynamic_config}}, AsyncIterable[_ResponseModelT]
    ]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[True] | StreamConfig,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: type[_ResponseModelT],
        output_parser: None = None,
        json_mode: bool = False,
        client: {{provider.sync_base_client}} = ...,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> SyncLLMFunctionDecorator[{{provider.base_dynamic_config}}, Iterable[_ResponseModelT]]: ...

    @overload
    def __call__(
        self,
        provider: Literal["{{provider.name}}"],
        model: str,
        *,
        stream: Literal[True] | StreamConfig,
        tools: list[type[BaseTool] | Callable] | None = None,
        response_model: type[_ResponseModelT],
        output_parser: Callable[
            [
                CallResponse[{{provider.response}}, {{provider.base_tool}}],
            ],
            _ParsedOutputT,
        ]
        | Callable[[CallResponseChunk[{{provider.response_chunk}}]], _ParsedOutputT]
        | Callable[[_ResponseModelT], _ParsedOutputT]
        | None,
        json_mode: bool = False,
        client: {{provider.same_sync_and_async_base_client}}
        | {{provider.async_base_client}}
        | {{provider.sync_base_client}}
        | None = None,
        call_params: {{provider.base_call_param}} | None = None,
    ) -> NoReturn: ...
{% endfor %}
    def __call__(
        self,
        provider: Any,
        model: Any,
        *,
        stream: Any = False,
        tools: Any = None,
        response_model: Any = None,
        output_parser: Any = None,
        json_mode: bool = False,
        client: Any = None,
        call_params: Any = None,
    ) ->Any: ...
